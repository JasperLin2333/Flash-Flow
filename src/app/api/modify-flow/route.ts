import { NextResponse } from "next/server";
export const runtime = 'edge';
import OpenAI from "openai";
import { PROVIDER_CONFIG, getProviderForModel } from "@/lib/llmProvider";
import { getAuthenticatedUser, unauthorizedResponse } from "@/lib/authEdge";
import { checkQuotaOnServer, incrementQuotaOnServer, quotaExceededResponse } from "@/lib/quotaEdge";
import { SMART_RULES, VARIABLE_RULES, NODE_SPECS, EDGE_RULES, CORE_CHECKLIST, EFFICIENCY_RULES } from "@/lib/prompts";
import { WorkflowZodSchema } from "@/lib/schemas/workflow";

// ============ Patch Mode Handler ============
async function handlePatchMode(
  prompt: string,
  currentNodes: any[],
  currentEdges: any[],
  client: OpenAI,
  model: string
) {
  // æ„å»ºç²¾ç®€çš„èŠ‚ç‚¹ä¸Šä¸‹æ–‡ï¼ˆåªæ˜¾ç¤ºå…³é”®ä¿¡æ¯ï¼‰
  const compactNodes = currentNodes.map(n => ({
    id: n.id,
    type: n.type,
    label: n.data?.label || n.type,
    // åªä¿ç•™å¯ä¿®æ”¹çš„æ ¸å¿ƒé…ç½®
    config: {
      ...(n.type === 'llm' && {
        model: n.data?.model,
        temperature: n.data?.temperature,
        enableMemory: n.data?.enableMemory,
        historyRounds: n.data?.historyRounds,
      }),
      ...(n.type === 'input' && {
        enableTextInput: n.data?.enableTextInput,
        enableFileInput: n.data?.enableFileInput,
      }),
      ...(n.type === 'imagegen' && {
        model: n.data?.model,
        creativity: n.data?.creativity,
      }),
    }
  }));

  const patchPrompt = `ä½ æ˜¯å·¥ä½œæµä¿®æ”¹ä¸“å®¶ã€‚æ ¹æ®ç”¨æˆ·éœ€æ±‚ï¼Œç²¾å‡†è¾“å‡ºéœ€è¦ä¿®æ”¹çš„èŠ‚ç‚¹é…ç½®ã€‚

# å½“å‰èŠ‚ç‚¹
\`\`\`json
${JSON.stringify(compactNodes, null, 2)}
\`\`\`

# ç”¨æˆ·éœ€æ±‚
${prompt}

# è¾“å‡ºè§„åˆ™ï¼ˆé‡è¦ï¼ï¼‰
1. **ä»…è¾“å‡ºéœ€è¦ä¿®æ”¹çš„å­—æ®µ**ï¼Œä¸è¦è¾“å‡ºå®Œæ•´å·¥ä½œæµ
2. ä½¿ç”¨ patches æ•°ç»„æ ¼å¼
3. nodeId å¿…é¡»ä½¿ç”¨èŠ‚ç‚¹çš„çœŸå® ID
4. data åªåŒ…å«éœ€è¦æ›´æ–°çš„å­—æ®µ

# è¾“å‡ºæ ¼å¼
\`\`\`json
{
  "patches": [
    { "nodeId": "node-xxx", "data": { "å­—æ®µ": "æ–°å€¼" } }
  ]
}
\`\`\`

å¦‚éœ€æ·»åŠ èŠ‚ç‚¹ï¼š
\`\`\`json
{
  "action": "add",
  "nodeType": "llm",
  "nodeData": { ... },
  "connectAfter": "ä¸Šæ¸¸èŠ‚ç‚¹ID"
}
\`\`\`

å¦‚éœ€åˆ é™¤èŠ‚ç‚¹ï¼š
\`\`\`json
{
  "action": "delete",
  "target": "è¦åˆ é™¤çš„èŠ‚ç‚¹ID"
}
\`\`\`
`;

  const completion = await client.chat.completions.create({
    model,
    temperature: 0.1,
    messages: [
      { role: "system", content: patchPrompt },
      { role: "user", content: "è¯·è¾“å‡º JSON patchesã€‚" },
    ],
    response_format: { type: "json_object" },
  });

  const content = completion.choices?.[0]?.message?.content || "{}";

  try {
    return JSON.parse(content);
  } catch {
    // è§£æå¤±è´¥ï¼Œè¿”å›ç©ºä»¥è§¦å‘ fallback
    return { error: "parse_failed" };
  }
}

// ============ Full Mode Handler (åŸæœ‰é€»è¾‘) ============
async function handleFullMode(
  prompt: string,
  currentNodes: any[],
  currentEdges: any[],
  client: OpenAI,
  model: string
) {
  const currentWorkflowJSON = JSON.stringify(
    { nodes: currentNodes, edges: currentEdges },
    null,
    2
  );

  const system = `ä½ æ˜¯å·¥ä½œæµä¿®æ”¹ä¸“å®¶ã€‚æ ¹æ®ç”¨æˆ·çš„ä¿®æ”¹éœ€æ±‚ï¼ŒåŸºäºå½“å‰å·¥ä½œæµä¸Šä¸‹æ–‡ï¼Œç²¾å‡†ç”Ÿæˆä¿®æ”¹åçš„å®Œæ•´ JSON å·¥ä½œæµã€‚

# ğŸ“‹ å½“å‰å·¥ä½œæµä¸Šä¸‹æ–‡
\`\`\`json
${currentWorkflowJSON}
\`\`\`

# ğŸ§  æ ¸å¿ƒåŸåˆ™

1. **æœ€å°æ”¹åŠ¨**: ä»…ä¿®æ”¹ç”¨æˆ·æ˜ç¡®è¦æ±‚çš„éƒ¨åˆ†ï¼Œä¿ç•™å…¶ä»–é…ç½®ä¸å˜ã€‚
2. **ç²¾å‡†å®šä½**: æ ¹æ®èŠ‚ç‚¹ label æˆ– type ç²¾å‡†å®šä½ç›®æ ‡èŠ‚ç‚¹ï¼ˆç¦æ­¢çŒœæµ‹ï¼‰ã€‚
3. **å®Œæ•´è¾“å‡º**: è¾“å‡ºä¿®æ”¹åçš„**å®Œæ•´å·¥ä½œæµ** JSONï¼ˆåŒ…å«æ‰€æœ‰èŠ‚ç‚¹å’Œè¾¹ï¼‰ã€‚

${SMART_RULES}

${EFFICIENCY_RULES}

## ğŸ¯ ä¿®æ”¹æ„å›¾è¯†åˆ«

| ç”¨æˆ·å¯èƒ½è¯´ | ä¿®æ”¹æ“ä½œ | ç›®æ ‡å®šä½ |
|-----------|---------|----------|
| "è®©å®ƒè®°ä½å¯¹è¯/åŠ è®°å¿†" | ä¿®æ”¹ LLM èŠ‚ç‚¹ | \`enableMemory: true\` |
| "æ›´å‡†ç¡®/æ›´ç¨³å®š" | ä¿®æ”¹ LLM èŠ‚ç‚¹ | \`temperature: 0.1-0.3\` |
| "åŠ ä¸Šæ–‡ä»¶ä¸Šä¼ /æ”¯æŒå›¾ç‰‡" | ä¿®æ”¹ Input èŠ‚ç‚¹ | \`enableFileInput: true\` |
| "åŠ ä¸ªåˆ†æ”¯/åˆ†æµå¤„ç†" | æ·»åŠ  Branch èŠ‚ç‚¹ | æ’å…¥åˆ°æŒ‡å®šä½ç½® |
| "åˆ æ‰è¿™ä¸ªèŠ‚ç‚¹" | åˆ é™¤èŠ‚ç‚¹ | åŒæ—¶åˆ é™¤ç›¸å…³è¾¹ |
| "æŠŠXXæ”¹æˆYY" | ä¿®æ”¹èŠ‚ç‚¹å±æ€§ | æ›´æ–° label/prompt ç­‰ |
| "åŠ ä¸ªæœç´¢åŠŸèƒ½" | æ·»åŠ  Tool èŠ‚ç‚¹ | \`toolType: web_search\` |

> ğŸ”µ **å®šä½è§„åˆ™**: ç”¨æˆ·è¯´"ç¿»è¯‘èŠ‚ç‚¹" â†’ æ‰¾ label åŒ…å«"ç¿»è¯‘"çš„èŠ‚ç‚¹ï¼›è¯´"LLM" â†’ æ‰¾ type=llm çš„èŠ‚ç‚¹


${VARIABLE_RULES}

${NODE_SPECS}

${EDGE_RULES}

# âœ… ä¿®æ”¹æ“ä½œæ£€æŸ¥æ¸…å•
1. âš ï¸ ä¿®æ”¹åçš„èŠ‚ç‚¹ ID å¿…é¡»ä¸åŸå·¥ä½œæµä¿æŒä¸€è‡´
2. âš ï¸ æ–°å¢èŠ‚ç‚¹éœ€æ­£ç¡®è¿æ¥ä¸Šä¸‹æ¸¸è¾¹
3. âš ï¸ åˆ é™¤èŠ‚ç‚¹æ—¶éœ€åŒæ—¶åˆ é™¤ç›¸å…³è¾¹

${CORE_CHECKLIST}

# è¾“å‡ºæ ¼å¼
è¾“å‡º**ä¿®æ”¹åçš„å®Œæ•´å·¥ä½œæµ** JSONï¼ˆä¿ç•™æœªä¿®æ”¹çš„èŠ‚ç‚¹ï¼‰ï¼š
\`\`\`json
{"title": "...", "nodes": [...], "edges": [...]}
\`\`\`
`;

  const finalSystemPrompt = system + "\\n\\n# ç”¨æˆ·è¯·æ±‚\\n" + prompt;
  const userMsg = "è¯·æŒ‰ç…§ system prompt ä¸­çš„è§„åˆ™è§£æç”¨æˆ·éœ€æ±‚å¹¶ç”Ÿæˆ JSON æŒ‡ä»¤ã€‚";

  const completion = await client.chat.completions.create({
    model,
    temperature: 0.1,
    messages: [
      { role: "system", content: finalSystemPrompt },
      { role: "user", content: userMsg },
    ],
    response_format: { type: "json_object" },
  });

  const content = completion.choices?.[0]?.message?.content || "{}";

  let jsonText = content;
  const match = content.match(/\{[\s\S]*\}/);
  if (match) jsonText = match[0];

  try {
    const instruction = JSON.parse(jsonText);

    // Validation logging
    const validation = WorkflowZodSchema.safeParse(instruction);
    if (!validation.success && process.env.NODE_ENV === 'development') {
      console.warn("Modify-Flow Schema Validation Failed:", validation.error);
    }

    return instruction;
  } catch {
    return { action: "unknown" };
  }
}

// ============ Main Handler ============
export async function POST(req: Request) {
  const reqClone = req.clone();

  try {
    // Authentication check
    const user = await getAuthenticatedUser(req);
    if (!user) {
      return unauthorizedResponse();
    }

    // Server-side quota check
    const quotaCheck = await checkQuotaOnServer(req, user.id, "flow_generations");
    if (!quotaCheck.allowed) {
      return quotaExceededResponse(quotaCheck.used, quotaCheck.limit, "Flow ç”Ÿæˆæ¬¡æ•°");
    }

    const body = await reqClone.json();
    // mode é»˜è®¤ä¸º "full" ä¿æŒå‘åå…¼å®¹
    const { prompt, currentNodes, currentEdges, mode = "full" } = body;

    if (!prompt || !currentNodes || !currentEdges) {
      return NextResponse.json({ error: "Missing required fields" }, { status: 400 });
    }

    // Dynamic provider resolution
    const defaultModel = process.env.DEFAULT_LLM_MODEL || "deepseek-ai/DeepSeek-V3.2";
    const provider = getProviderForModel(defaultModel);
    const config = PROVIDER_CONFIG[provider];

    const client = new OpenAI({
      apiKey: config.getApiKey(),
      baseURL: config.baseURL
    });

    // æ ¹æ® mode é€‰æ‹©å¤„ç†æ–¹å¼
    let result: any;
    if (mode === "patch") {
      result = await handlePatchMode(prompt, currentNodes, currentEdges, client, defaultModel);

      // å¦‚æœ patch æ¨¡å¼è§£æå¤±è´¥ï¼Œè¿”å›ç‰¹æ®Šæ ‡è®°è®©å‰ç«¯ fallback
      if (result.error === "parse_failed") {
        result = await handleFullMode(prompt, currentNodes, currentEdges, client, defaultModel);
      }
    } else {
      // Full modeï¼ˆåŸæœ‰é€»è¾‘ï¼‰
      result = await handleFullMode(prompt, currentNodes, currentEdges, client, defaultModel);
    }

    // Increment quota after successful modification
    await incrementQuotaOnServer(req, user.id, "flow_generations");

    return NextResponse.json(result);
  } catch (e) {
    if (process.env.NODE_ENV === 'development') {
      console.error("Modify flow error:", e);
    }
    return NextResponse.json({ error: "Failed to process modification" }, { status: 500 });
  }
}