import { NextResponse } from "next/server";
export const runtime = 'edge';
import OpenAI from "openai";
import { PROVIDER_CONFIG, getProviderForModel } from "@/lib/llmProvider";
import { getAuthenticatedUser, unauthorizedResponse } from "@/lib/authEdge";
import { checkQuotaOnServer, incrementQuotaOnServer, quotaExceededResponse } from "@/lib/quotaEdge";
import { CORE_RULES, MODIFY_PROMPT, NODE_REFERENCE, VARIABLE_RULES, EDGE_RULES } from "@/lib/prompts";
import { WorkflowZodSchema } from "@/lib/schemas/workflow";

// ============ Patch Mode Handler ============
async function handlePatchMode(
  prompt: string,
  currentNodes: any[],
  currentEdges: any[],
  client: OpenAI,
  model: string
) {
  // æ„å»ºç²¾ç®€çš„èŠ‚ç‚¹ä¸Šä¸‹æ–‡ï¼ˆåªæ˜¾ç¤ºå…³é”®ä¿¡æ¯ï¼‰
  const compactNodes = currentNodes.map(n => ({
    id: n.id,
    type: n.type,
    label: n.data?.label || n.type,
    // åªä¿ç•™å¯ä¿®æ”¹çš„æ ¸å¿ƒé…ç½®
    config: {
      ...(n.type === 'llm' && {
        model: n.data?.model,
        temperature: n.data?.temperature,
        enableMemory: n.data?.enableMemory,
        memoryMaxTurns: n.data?.memoryMaxTurns,
        responseFormat: n.data?.responseFormat,
        systemPrompt: n.data?.systemPrompt?.slice(0, 100) + '...',
      }),
      ...(n.type === 'input' && {
        enableTextInput: n.data?.enableTextInput,
        enableFileInput: n.data?.enableFileInput,
        enableStructuredForm: n.data?.enableStructuredForm,
      }),
      ...(n.type === 'rag' && {
        fileMode: n.data?.fileMode,
        inputMappings: n.data?.inputMappings,
      }),
      ...(n.type === 'imagegen' && {
        model: n.data?.model,
        cfg: n.data?.cfg,
        numInferenceSteps: n.data?.numInferenceSteps,
        referenceImageMode: n.data?.referenceImageMode,
      }),
      ...(n.type === 'branch' && {
        condition: n.data?.condition,
      }),
      ...(n.type === 'tool' && {
        toolType: n.data?.toolType,
      }),
    }
  }));

  const patchPrompt = `ä½ æ˜¯å·¥ä½œæµä¿®æ”¹ä¸“å®¶ã€‚æ ¹æ®ç”¨æˆ·éœ€æ±‚ï¼Œç²¾å‡†è¾“å‡ºéœ€è¦ä¿®æ”¹çš„èŠ‚ç‚¹é…ç½®ã€‚

# å½“å‰èŠ‚ç‚¹ (ç²¾ç®€ç‰ˆ)
\`\`\`json
${JSON.stringify(compactNodes, null, 2)}
\`\`\`

# ç”¨æˆ·éœ€æ±‚
${prompt}

# ä¿®æ”¹æŒ‡å— (Intent Mapping)
| ç”¨æˆ·æ„å›¾ | ç›®æ ‡èŠ‚ç‚¹ | ä¿®æ”¹å»ºè®® (å‚è€ƒ) |
|---------|---------|----------------|
| **"åŠ è®°å¿†"** | LLM | \`enableMemory: true\`, \`memoryMaxTurns: 10\` |
| **"æ›´ä¸¥è°¨"** | LLM | \`temperature: 0.1\` |
| **"æ›´æœ‰åˆ›æ„"** | LLM | \`temperature: 0.9\` |
| **"åˆ‡æ¢æ¨¡å‹"** | LLM | \`model: "deepseek-reasoner"\` (å¦‚éœ€æ¨ç†) |
| **"è¾“å‡º JSON"** | LLM | \`responseFormat: "json_object"\` (åŠ¡å¿…åŒæ—¶ä¿®æ”¹ SystemPrompt) |
| **"ä¸Šä¼ æ–‡ä»¶"** | Input | \`enableFileInput: true\`, \`fileConfig: { allowedTypes: [".pdf"], ... }\` |
| **"æœé›†è¡¨å•"** | Input | \`enableStructuredForm: true\`, \`formFields: [...]\` |
| **"ä¿®æ”¹åˆ†æ”¯"** | Branch | \`condition: "{{A.score}} > 60"\` (ç¡®ä¿ä½¿ç”¨ {{}} å¼•ç”¨å˜é‡) |

# è¾“å‡ºè§„åˆ™ (Strict Rules)
1. **æœ€å°ä¿®æ”¹åŸåˆ™**: ä»…è¾“å‡ºéœ€è¦å˜æ›´çš„å­—æ®µã€‚
2. **ID ç»å¯¹ä¸€è‡´**: \`nodeId\` å¿…é¡»ç²¾å‡†å¯¹åº”ä¸Šæ–¹æä¾›çš„èŠ‚ç‚¹ IDã€‚
3. **LLM æç¤ºè¯è§„èŒƒ**: è‹¥ä¿®æ”¹ SystemPromptï¼Œå¿…é¡»ä½¿ç”¨ Markdown æ ¼å¼ (Role/Task/Constraints)ã€‚
4. **æ•°æ®ç±»å‹**: ä¸¥æ ¼éµå®ˆ TypeScript å®šä¹‰ (e.g. numeric fields must be numbers).

# è¾“å‡ºæ ¼å¼ (JSON Patches)
\`\`\`json
{
  "patches": [
    { "nodeId": "llm_main", "data": { "temperature": 0.2 } },
    { "nodeId": "input_root", "data": { "greeting": "æ¬¢è¿å’¨è¯¢ï¼" } }
  ]
}
\`\`\`

# æ·»åŠ èŠ‚ç‚¹ (Add Action)
\`\`\`json
{
  "action": "add",
  "nodeType": "tool",
  "nodeData": {
    "label": "è”ç½‘æœç´¢",
    "toolType": "web_search",
    "inputs": { "query": "{{Input.user_input}}", "maxResults": 5 }
  },
  "connectAfter": "parent_node_id" // å°†æ’å…¥åœ¨æ­¤èŠ‚ç‚¹ä¹‹å
}
\`\`\`

# åˆ é™¤èŠ‚ç‚¹ (Delete Action)
\`\`\`json
{
  "action": "delete",
  "target": "node_id_to_delete"
}
\`\`\`
`;

  const completion = await client.chat.completions.create({
    model,
    temperature: 0.1,
    messages: [
      { role: "system", content: patchPrompt },
      { role: "user", content: "è¯·åˆ†æéœ€æ±‚å¹¶ç”Ÿæˆ JSON æŒ‡ä»¤ã€‚" },
    ],
    response_format: { type: "json_object" },
  });

  const content = completion.choices?.[0]?.message?.content || "{}";

  try {
    return JSON.parse(content);
  } catch {
    // è§£æå¤±è´¥ï¼Œè¿”å›ç©ºä»¥è§¦å‘ fallback
    return { error: "parse_failed" };
  }
}

// ============ Full Mode Handler (å¤ç”¨ plan æç¤ºè¯ç»“æ„) ============
async function handleFullMode(
  prompt: string,
  currentNodes: any[],
  currentEdges: any[],
  client: OpenAI,
  model: string
) {
  const currentWorkflowJSON = JSON.stringify(
    { nodes: currentNodes, edges: currentEdges },
    null,
    2
  );

  // å¤ç”¨ plan/route.ts çš„æç¤ºè¯ç»“æ„ï¼Œæ·»åŠ ä¿®æ”¹ä¸“ç”¨ä¸Šä¸‹æ–‡
  const system = `ä½ æ˜¯å·¥ä½œæµä¿®æ”¹ä¸“å®¶ã€‚æ ¹æ®ç”¨æˆ·çš„ä¿®æ”¹éœ€æ±‚ï¼ŒåŸºäºå½“å‰å·¥ä½œæµä¸Šä¸‹æ–‡ï¼Œç²¾å‡†ç”Ÿæˆä¿®æ”¹åçš„å®Œæ•´ JSON å·¥ä½œæµã€‚

# ğŸ“‹ å½“å‰å·¥ä½œæµä¸Šä¸‹æ–‡
\`\`\`json
${currentWorkflowJSON}
\`\`\`

# ğŸ§  æ ¸å¿ƒåŸåˆ™ (Modification Principles)

1. **æœ€å°æ”¹åŠ¨ (Minimalism)**: ä»…ä¿®æ”¹ç”¨æˆ·æ˜ç¡®è¦æ±‚çš„éƒ¨åˆ†ï¼Œä¸¥ç¦éšæ„é‡æ„æœªæåŠçš„é€»è¾‘ã€‚
2. **ç²¾å‡†å®šä½ (Targeting)**: æ ¹æ® label æˆ– type é”å®šç›®æ ‡èŠ‚ç‚¹ã€‚
   - ç”¨æˆ·è¯´ "ç¿»è¯‘èŠ‚ç‚¹" -> åŒ¹é… label="ç¿»è¯‘"
   - ç”¨æˆ·è¯´ "LLM" -> åŒ¹é… type="llm"
3. **ID ä¿æŒ (Identity Preservation)**: å¿…é¡»ä¿ç•™åŸæœ‰èŠ‚ç‚¹çš„ IDï¼Œç¡®ä¿å‰ç«¯è§†å›¾ç¨³å®šã€‚
4. **å®Œæ•´é—­ç¯ (Completeness)**: è¾“å‡ºå¿…é¡»æ˜¯å®Œæ•´çš„ JSON (nodes + edges)ï¼ŒåŒ…å«æ‰€æœ‰æœªä¿®æ”¹çš„èŠ‚ç‚¹ã€‚

${MODIFY_PROMPT}

${CORE_RULES}

${NODE_REFERENCE}

${VARIABLE_RULES}

${EDGE_RULES}

# âœ… ä¿®æ”¹æ£€æŸ¥æ¸…å• (Sanity Check)
1. âš ï¸ **è¿çº¿å®Œæ•´æ€§**: æ–°å¢èŠ‚ç‚¹æ˜¯å¦å·²æ­£ç¡®è¿æ¥ï¼Ÿåˆ é™¤èŠ‚ç‚¹æ˜¯å¦æ¸…ç†äº†æ‚¬ç©ºè¾¹ï¼Ÿ
2. âš ï¸ **å˜é‡å¼•ç”¨**: ä¿®æ”¹å¼•ç”¨æ—¶æ˜¯å¦ä½¿ç”¨äº†æ­£ç¡®çš„ label? (e.g. \`{{Label.field}}\`)
3. âš ï¸ **LLMé…ç½®**: æ˜¯å¦ä¸º LLM èŠ‚ç‚¹é…ç½®äº† \`inputMappings.user_input\`?



# è¾“å‡ºæ ¼å¼
è¾“å‡º**ä¿®æ”¹åçš„å®Œæ•´å·¥ä½œæµ** JSONï¼ˆä¿ç•™æœªä¿®æ”¹çš„èŠ‚ç‚¹ï¼‰ï¼š
\`\`\`json
{"title": "...", "nodes": [...], "edges": [...]}
\`\`\`
`;

  const finalSystemPrompt = system + "\\n\\n# ç”¨æˆ·è¯·æ±‚\\n" + prompt;
  const userMsg = "è¯·æŒ‰ç…§ system prompt ä¸­çš„è§„åˆ™è§£æç”¨æˆ·éœ€æ±‚å¹¶ç”Ÿæˆ JSON æŒ‡ä»¤ã€‚";

  const completion = await client.chat.completions.create({
    model,
    temperature: 0.1,
    messages: [
      { role: "system", content: finalSystemPrompt },
      { role: "user", content: userMsg },
    ],
    response_format: { type: "json_object" },
  });

  const content = completion.choices?.[0]?.message?.content || "{}";

  let jsonText = content;
  const match = content.match(/\{[\s\S]*\}/);
  if (match) jsonText = match[0];

  try {
    const instruction = JSON.parse(jsonText);

    // Validation logging
    const validation = WorkflowZodSchema.safeParse(instruction);
    if (!validation.success && process.env.NODE_ENV === 'development') {
      console.warn("Modify-Flow Schema Validation Failed:", validation.error);
    }

    return instruction;
  } catch {
    return { action: "unknown" };
  }
}

// ============ Main Handler ============
export async function POST(req: Request) {
  const reqClone = req.clone();

  try {
    // Authentication check
    const user = await getAuthenticatedUser(req);
    if (!user) {
      return unauthorizedResponse();
    }

    // Server-side quota check
    const quotaCheck = await checkQuotaOnServer(req, user.id, "flow_generations");
    if (!quotaCheck.allowed) {
      return quotaExceededResponse(quotaCheck.used, quotaCheck.limit, "Flow ç”Ÿæˆæ¬¡æ•°");
    }

    const body = await reqClone.json();
    // mode é»˜è®¤ä¸º "full" ä¿æŒå‘åå…¼å®¹
    const { prompt, currentNodes, currentEdges, mode = "full" } = body;

    if (!prompt || !currentNodes || !currentEdges) {
      return NextResponse.json({ error: "Missing required fields" }, { status: 400 });
    }

    // Dynamic provider resolution
    const defaultModel = process.env.DEFAULT_LLM_MODEL || "deepseek-ai/DeepSeek-V3.2";
    const provider = getProviderForModel(defaultModel);
    const config = PROVIDER_CONFIG[provider];

    const client = new OpenAI({
      apiKey: config.getApiKey(),
      baseURL: config.baseURL
    });

    // æ ¹æ® mode é€‰æ‹©å¤„ç†æ–¹å¼
    let result: any;
    if (mode === "patch") {
      result = await handlePatchMode(prompt, currentNodes, currentEdges, client, defaultModel);

      // å¦‚æœ patch æ¨¡å¼è§£æå¤±è´¥ï¼Œè¿”å›ç‰¹æ®Šæ ‡è®°è®©å‰ç«¯ fallback
      if (result.error === "parse_failed") {
        result = await handleFullMode(prompt, currentNodes, currentEdges, client, defaultModel);
      }
    } else {
      // Full modeï¼ˆåŸæœ‰é€»è¾‘ï¼‰
      result = await handleFullMode(prompt, currentNodes, currentEdges, client, defaultModel);
    }

    // Increment quota after successful modification
    await incrementQuotaOnServer(req, user.id, "flow_generations");

    return NextResponse.json(result);
  } catch (e) {
    if (process.env.NODE_ENV === 'development') {
      console.error("Modify flow error:", e);
    }
    return NextResponse.json({ error: "Failed to process modification" }, { status: 500 });
  }
}