import OpenAI from "openai";
export const runtime = 'edge';
import { PlanRequestSchema } from "@/utils/validation";
import { PROVIDER_CONFIG, getProviderForModel } from "@/lib/llmProvider";
import { getAuthenticatedUser, unauthorizedResponse } from "@/lib/authEdge";
import { checkQuotaOnServer, incrementQuotaOnServer, quotaExceededResponse } from "@/lib/quotaEdge";
import { SMART_RULES, VARIABLE_RULES, NODE_SPECS, EDGE_RULES, SCENARIO_RULES, CORE_CHECKLIST, EFFICIENCY_RULES } from "@/lib/prompts";
import { WorkflowZodSchema } from "@/lib/schemas/workflow";

// ============ å…œåº•ç­–ç•¥é…ç½® ============
const FALLBACK_MODEL = "gemini-3-flash-preview"; // å¤‡é€‰æ¨¡å‹ (è§†è§‰+æ–‡æœ¬)
const MAX_RETRIES = 2; // æ¯ä¸ªæ¨¡å‹æœ€å¤§é‡è¯•æ¬¡æ•°
const RETRY_DELAY_MS = 1000; // é‡è¯•å»¶è¿Ÿ

/** å»¶è¿Ÿå‡½æ•° */
const delay = (ms: number) => new Promise(resolve => setTimeout(resolve, ms));

/** åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•ï¼ˆå¯æ¢å¤æ€§é”™è¯¯ï¼‰ */
function shouldRetry(error: unknown): boolean {
  const msg = error instanceof Error ? error.message.toLowerCase() : String(error).toLowerCase();
  // è¶…æ—¶ã€é€Ÿç‡é™åˆ¶ã€æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ â†’ é‡è¯•
  return msg.includes("timeout") ||
    msg.includes("rate limit") ||
    msg.includes("429") ||
    msg.includes("503") ||
    msg.includes("502") ||
    msg.includes("network") ||
    msg.includes("econnreset") ||
    msg.includes("fetch failed");
}

/** åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆ‡æ¢åˆ°å¤‡é€‰æ¨¡å‹ */
function shouldFallback(error: unknown): boolean {
  const msg = error instanceof Error ? error.message.toLowerCase() : String(error).toLowerCase();
  // 5xx é”™è¯¯ï¼ˆéæš‚æ—¶æ€§ï¼‰ã€æ¨¡å‹ä¸å¯ç”¨ â†’ åˆ‡æ¢å¤‡é€‰
  return msg.includes("500") ||
    msg.includes("model not found") ||
    msg.includes("invalid model") ||
    msg.includes("unsupported");
}


export async function POST(req: Request) {
  // Clone request for quota operations
  const reqClone = req.clone();

  try {
    // Authentication check
    const user = await getAuthenticatedUser(req);
    if (!user) {
      return unauthorizedResponse();
    }

    // Server-side quota check for flow generations
    const quotaCheck = await checkQuotaOnServer(req, user.id, "flow_generations");
    if (!quotaCheck.allowed) {
      return quotaExceededResponse(quotaCheck.used, quotaCheck.limit, "Flow ç”Ÿæˆæ¬¡æ•°");
    }

    const body = await reqClone.json();

    // 1. Validation
    const parseResult = PlanRequestSchema.safeParse(body);
    if (!parseResult.success) {
      return new Response(
        JSON.stringify({ error: "Invalid input", details: parseResult.error.format() }),
        { status: 400, headers: { "Content-Type": "application/json" } }
      );
    }
    const { prompt } = parseResult.data;

    // 2. Early return for empty prompt
    if (!prompt.trim()) {
      return new Response(
        JSON.stringify({ nodes: [], edges: [] }),
        { headers: { "Content-Type": "application/json" } }
      );
    }

    // Files placeholder - knowledge base files are configured in the UI, not passed from frontend
    const files: { name: string; size?: number; type?: string }[] = [];

    // 3. Model configuration (reads from environment variable for easy updates)
    const preferredModel = process.env.DEFAULT_LLM_MODEL || "deepseek-ai/DeepSeek-V3.2";

    // Import shared prompt modules
    // Note: Constants are imported from '@/lib/prompts' at the top of the file

    const system = `ä½ æ˜¯å·¥ä½œæµç¼–æ’ä¸“å®¶ã€‚æ ¹æ®ç”¨æˆ·éœ€æ±‚æè¿°ï¼Œæ™ºèƒ½ç”Ÿæˆå®Œæ•´çš„ JSON å·¥ä½œæµã€‚

# ğŸ§  æ ¸å¿ƒåŸåˆ™

1. **é€»è¾‘æ·±åº¦**: LLM SystemPrompt å¿…é¡»åŒ…å«å…·ä½“çš„æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ï¼ˆè§’è‰²/ç›®æ ‡/çº¦æŸï¼‰ï¼Œæ‹’ç»ç©ºæ´å†…å®¹ã€‚
2. **åœºæ™¯é€‚é…**: æ ¹æ®éœ€æ±‚ç²¾å‡†é€‰æ‹©èŠ‚ç‚¹ç»„åˆå’Œå‚æ•°ã€‚
3. **æ¨¡ç³Šå…œåº•**: éœ€æ±‚ä¸æ˜ç¡®æ—¶ï¼Œä¼˜å…ˆç”Ÿæˆ Input â†’ LLM â†’ Output ä¸‰èŠ‚ç‚¹ç›´é“¾ï¼Œåœ¨ LLM çš„ systemPrompt ä¸­å¼•å¯¼ç”¨æˆ·è¡¥å……ä¿¡æ¯ã€‚

${EFFICIENCY_RULES}

${SMART_RULES}

${SCENARIO_RULES}

${VARIABLE_RULES}

${NODE_SPECS}

${EDGE_RULES}

# ğŸ“‹ å…³é”®ç¤ºä¾‹

## 1. ğŸ–¼ï¸ å›¾ç‰‡åˆ†æ (Vision)
\`\`\`json
{"title": "å·¥å•OCRè¯†åˆ«", "nodes": [
  {"id": "in", "type": "input", "data": {"label": "ä¸Šä¼ å·¥å•", "enableFileInput": true, "fileConfig": {"allowedTypes": [".jpg",".png",".webp"], "maxCount": 1}}},
  {"id": "llm", "type": "llm", "data": {"label": "æ™ºèƒ½è¯†åˆ«", "model": "deepseek-ai/DeepSeek-OCR", "temperature": 0.1, "enableMemory": false, "systemPrompt": "# è§’è‰²\\nä½ æ˜¯å·¥å•è¯†åˆ«ä¸“å®¶ï¼Œç²¾é€šç»´ä¿®å·¥å•ã€ç‰©æµå•æ®çš„ç»“æ„åŒ–æå–ã€‚\\n\\n# ä»»åŠ¡\\nåˆ†æå›¾ç‰‡ {{ä¸Šä¼ å·¥å•.files}}ï¼Œæå–å…³é”®å­—æ®µã€‚\\n\\n# è¾“å‡ºæ ¼å¼ (JSON)\\n{\\\"å•å·\\\": \\\"..\\\", \\\"æ—¥æœŸ\\\": \\\"YYYY-MM-DD\\\", \\\"å®¢æˆ·\\\": \\\"..\\\", \\\"æ•…éšœæè¿°\\\": \\\"..\\\", \\\"çŠ¶æ€\\\": \\\"å¾…å¤„ç†|å·²å®Œæˆ\\\"}\\n\\n# çº¦æŸ\\n- æ¨¡ç³Šå­—æ®µæ ‡æ³¨ [æ— æ³•è¯†åˆ«]\\n- æ—¥æœŸè½¬ ISO æ ¼å¼\\"}},
  {"id": "out", "type": "output", "data": {"label": "è¯†åˆ«ç»“æœ", "inputMappings": {"mode": "direct", "sources": [{"type": "variable", "value": "{{æ™ºèƒ½è¯†åˆ«.response}}"}]}}}
], "edges": [{"source": "in", "target": "llm"}, {"source": "llm", "target": "out"}]}
\`\`\`

## 2. ğŸ’° æ™ºèƒ½ç†è´¢ (Branch + Tool + ç»“æ„åŒ–è¡¨å•)
\`\`\`json
{"title": "æ™ºèƒ½ç†è´¢é¡¾é—®", "nodes": [
  {"id": "in", "type": "input", "data": {"label": "æŠ•èµ„åå¥½", "enableStructuredForm": true, "formFields": [{"name": "risk", "label": "é£é™©åå¥½", "type": "select", "options": ["ä¿å®ˆå‹", "æ¿€è¿›å‹"], "required": true}]}},
  {"id": "br", "type": "branch", "data": {"label": "ç­–ç•¥åˆ†æµ", "condition": "æŠ•èµ„åå¥½.formData.risk === 'ä¿å®ˆå‹'"}},
  {"id": "t_bond", "type": "tool", "data": {"label": "æŸ¥è¯¢å›½å€º", "toolType": "web_search", "inputs": {"query": "2024å¹´å›½å€ºåˆ©ç‡ æœ€æ–°æ”¶ç›Šç‡"}}},
  {"id": "t_stock", "type": "tool", "data": {"label": "æŸ¥è¯¢ç¾è‚¡", "toolType": "web_search", "inputs": {"query": "çº³æ–¯è¾¾å…‹ ç§‘æŠ€è‚¡ æœ¬å‘¨æ¶¨å¹…æ¦œ"}}},
  {"id": "llm_safe", "type": "llm", "data": {"label": "ç¨³å¥æ–¹æ¡ˆ", "temperature": 0.3, "systemPrompt": "# è§’è‰²\\nä½ æ˜¯ CFA è®¤è¯çš„ä¿å®ˆå‹ç†è´¢é¡¾é—®ï¼Œä¸“æ³¨æœ¬é‡‘å®‰å…¨ã€‚\\n\\n# ä»»åŠ¡\\nåŸºäºå›½å€ºä¿¡æ¯ {{æŸ¥è¯¢å›½å€º.results}} åˆ¶å®šç†è´¢æ–¹æ¡ˆã€‚\\n\\n# è¾“å‡ºè¦æ±‚\\n1. **æ¨èäº§å“**: 2-3ä¸ªä½é£é™©äº§å“åŠé¢„æœŸå¹´åŒ–\\n2. **é…ç½®å»ºè®®**: å¦‚ å›½å€º60%+è´§åŸº40%\\n3. **é£é™©æç¤º**: æœ¬é‡‘æ³¢åŠ¨èŒƒå›´\\n\\n# çº¦æŸ\\n- å¹´åŒ–ä¸è¶…5%\\n- ç¦æ­¢æ¨èè‚¡ç¥¨æœŸè´§\\"}},
  {"id": "llm_risk", "type": "llm", "data": {"label": "æ¿€è¿›æ–¹æ¡ˆ", "temperature": 0.7, "systemPrompt": "# è§’è‰²\\nä½ æ˜¯ä¸“æ³¨æˆé•¿è‚¡çš„æ¿€è¿›å‹æŠ•èµ„é¡¾é—®ã€‚\\n\\n# ä»»åŠ¡\\nåŸºäºç¾è‚¡ä¿¡æ¯ {{æŸ¥è¯¢ç¾è‚¡.results}} åˆ¶å®šæŠ•èµ„æ–¹æ¡ˆã€‚\\n\\n# è¾“å‡ºè¦æ±‚\\n1. **æ¨èæ ‡çš„**: 3-5åªé«˜æ½œåŠ›è‚¡åŠç†ç”±\\n2. **ä»“ä½ç­–ç•¥**: åˆ†æ‰¹å»ºä»“è®¡åˆ’\\n3. **æ­¢æŸç­–ç•¥**: æ˜ç¡®æ­¢æŸç‚¹ä½(-15%)\\n\\n# çº¦æŸ\\n- å¿…é¡»åŒ…å«é£é™©è­¦ç¤º\\n- å•åªä»“ä½â‰¤20%\\"}},
  {"id": "out", "type": "output", "data": {"label": "æŠ•èµ„æ–¹æ¡ˆ", "inputMappings": {"mode": "select", "sources": [{"type": "variable", "value": "{{ç¨³å¥æ–¹æ¡ˆ.response}}"}, {"type": "variable", "value": "{{æ¿€è¿›æ–¹æ¡ˆ.response}}"}]}}}
], "edges": [
  {"source": "in", "target": "br"},
  {"source": "br", "target": "t_bond", "sourceHandle": "true"}, {"source": "br", "target": "t_stock", "sourceHandle": "false"},
  {"source": "t_bond", "target": "llm_safe"}, {"source": "t_stock", "target": "llm_risk"},
  {"source": "llm_safe", "target": "out"}, {"source": "llm_risk", "target": "out"}
]}
\`\`\`

${CORE_CHECKLIST}

# è¾“å‡ºæ ¼å¼
çº¯ JSONï¼š
\`\`\`json
{"title": "...", "nodes": [...], "edges": [...]}
\`\`\`
`;

    const userMsg = [
      `ç”¨æˆ·æè¿°: ${prompt}`,
      files.length ? `å¯ç”¨çŸ¥è¯†åº“æ–‡ä»¶: ${files.map(f => f.name).join(", ")}` : "æ— å¯ç”¨çŸ¥è¯†åº“æ–‡ä»¶",
    ].join("\n");

    // Create streaming response to avoid timeout
    const encoder = new TextEncoder();

    const stream = new ReadableStream({
      async start(controller) {
        const modelsToTry = [preferredModel, FALLBACK_MODEL];
        let lastError: unknown = null;
        let success = false;

        // å°è¯•æ¯ä¸ªæ¨¡å‹
        for (let modelIndex = 0; modelIndex < modelsToTry.length && !success; modelIndex++) {
          const currentModel = modelsToTry[modelIndex];
          const isFallback = modelIndex > 0;

          // é€šçŸ¥åˆ‡æ¢åˆ°å¤‡é€‰æ¨¡å‹
          if (isFallback) {
            controller.enqueue(encoder.encode(`data: ${JSON.stringify({ type: "fallback", model: currentModel })}\n\n`));
          }

          // æ¯ä¸ªæ¨¡å‹æœ€å¤šé‡è¯• MAX_RETRIES æ¬¡
          for (let attempt = 0; attempt < MAX_RETRIES && !success; attempt++) {
            try {
              // é€šçŸ¥é‡è¯•
              if (attempt > 0) {
                controller.enqueue(encoder.encode(`data: ${JSON.stringify({ type: "retrying", attempt: attempt + 1, model: currentModel })}\n\n`));
                await delay(RETRY_DELAY_MS);
              }

              const provider = getProviderForModel(currentModel);
              const config = PROVIDER_CONFIG[provider];

              const client = new OpenAI({
                apiKey: config.getApiKey(),
                baseURL: config.baseURL
              });

              const completion = await client.chat.completions.create({
                model: currentModel,
                temperature: 0.2,
                messages: [
                  { role: "system", content: system },
                  { role: "user", content: userMsg },
                ],
                stream: true,
                response_format: { type: "json_object" }, // Added response_format
              });

              let fullContent = "";

              // Send progress updates to keep connection alive
              for await (const chunk of completion) {
                const content = chunk.choices?.[0]?.delta?.content || "";
                if (content) {
                  fullContent += content;
                  controller.enqueue(encoder.encode(`data: ${JSON.stringify({ type: "progress", content })}\n\n`));
                }
              }

              // Parse the complete response
              let jsonText = fullContent;
              const match = fullContent.match(/\{[\s\S]*\}/);
              if (match) jsonText = match[0];

              let plan: { title?: string; nodes?: unknown; edges?: unknown } = {};
              try {
                plan = JSON.parse(jsonText) as { title?: string; nodes?: unknown; edges?: unknown };
              } catch (parseError) {
                // JSON è§£æå¤±è´¥ï¼Œå¯èƒ½éœ€è¦é‡è¯•
                lastError = new Error("Failed to parse LLM response as JSON");
                if (shouldRetry(lastError) && attempt < MAX_RETRIES - 1) {
                  continue; // é‡è¯•å½“å‰æ¨¡å‹
                }
                // åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæ¨¡å‹
                break;
              }

              const title = plan?.title || prompt.slice(0, 20);
              const nodes = Array.isArray(plan?.nodes) ? plan.nodes : [];
              const edges = Array.isArray(plan?.edges) ? plan.edges : [];

              // æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†æœ‰æ•ˆå†…å®¹
              if (nodes.length === 0) {
                lastError = new Error("LLM returned empty nodes");
                if (attempt < MAX_RETRIES - 1) {
                  continue; // é‡è¯•
                }
                break; // åˆ‡æ¢æ¨¡å‹
              }

              // æˆåŠŸï¼å‘é€ç»“æœ
              controller.enqueue(encoder.encode(`data: ${JSON.stringify({ type: "result", title, nodes, edges })}\n\n`));
              await incrementQuotaOnServer(req, user.id, "flow_generations");
              success = true;

            } catch (error) {
              lastError = error;
              if (process.env.NODE_ENV === 'development') {
                console.error(`Plan generation error (model: ${currentModel}, attempt: ${attempt + 1}):`, error);
              }

              // åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•å½“å‰æ¨¡å‹
              if (shouldRetry(error) && attempt < MAX_RETRIES - 1) {
                continue; // é‡è¯•
              }

              // åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆ‡æ¢åˆ°å¤‡é€‰æ¨¡å‹
              if (shouldFallback(error) || attempt >= MAX_RETRIES - 1) {
                break; // è·³å‡ºé‡è¯•å¾ªç¯ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
              }
            }
          }
        }

        // æ‰€æœ‰å°è¯•éƒ½å¤±è´¥
        if (!success) {
          if (process.env.NODE_ENV === 'development') {
            console.error("All plan generation attempts failed:", lastError);
          }
          controller.enqueue(encoder.encode(`data: ${JSON.stringify({ type: "error", message: lastError instanceof Error ? lastError.message : "ç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åé‡è¯•" })}\n\n`));
          controller.enqueue(encoder.encode(`data: ${JSON.stringify({ type: "result", title: prompt.slice(0, 20), nodes: [], edges: [] })}\n\n`));
        }

        controller.enqueue(encoder.encode("data: [DONE]\n\n"));
        controller.close();
      }
    });

    return new Response(stream, {
      headers: {
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
      },
    });
  } catch (e) {
    if (process.env.NODE_ENV === 'development') {
      console.error("Plan API error:", e);
    }
    return new Response(
      JSON.stringify({ nodes: [], edges: [] }),
      { status: 200, headers: { "Content-Type": "application/json" } }
    );
  }
}

