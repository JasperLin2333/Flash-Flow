import type { AppNode, AppEdge, LLMNodeData, FlowContext } from "@/types/flow";
import { BaseNodeExecutor, type ExecutionResult } from "./BaseNodeExecutor";
import { replaceVariables } from "@/lib/promptParser";
import { quotaService } from "@/services/quotaService";
import { authService } from "@/services/authService";
import { llmMemoryService, type ConversationMessage } from "@/services/llmMemoryService";
import { extractInputFromContext } from "./contextUtils";
import { LLM_EXECUTOR_CONFIG } from "../constants/executorConfig";
import { useFlowStore } from "@/store/flowStore";
import { useQuotaStore } from "@/store/quotaStore";

/**
 * æ£€æŸ¥ LLM èŠ‚ç‚¹æ˜¯å¦ä¸ºç”¨æˆ·äº¤äº’èŠ‚ç‚¹
 * 
 * ç”¨æˆ·äº¤äº’ LLM çš„åˆ¤æ–­æ ‡å‡†:
 * 1. ç›´æ¥è¿æ¥åˆ° output èŠ‚ç‚¹
 * 2. ä» branch èŠ‚ç‚¹æ¥æ”¶è¾“å…¥ï¼ˆå¤šè·¯åˆ†æ”¯åçš„å¤„ç†èŠ‚ç‚¹ï¼‰
 * 
 * @param nodeId - å½“å‰ LLM èŠ‚ç‚¹çš„ ID
 * @param nodes - æ‰€æœ‰èŠ‚ç‚¹
 * @param edges - æ‰€æœ‰è¾¹
 * @returns æ˜¯å¦ä¸ºç”¨æˆ·äº¤äº’ LLM
 */
function checkIsUserFacingLLM(
  nodeId: string,
  nodes: AppNode[],
  edges: AppEdge[]
): boolean {
  // æ£€æŸ¥æ˜¯å¦ç›´æ¥è¿æ¥åˆ° output
  const outgoingEdges = edges.filter(e => e.source === nodeId);
  for (const edge of outgoingEdges) {
    const targetNode = nodes.find(n => n.id === edge.target);
    if (targetNode?.type === 'output') return true;
  }

  // æ£€æŸ¥æ˜¯å¦åœ¨ branch ä¹‹åï¼ˆä» branch æ¥æ”¶è¾“å…¥ï¼‰
  const incomingEdges = edges.filter(e => e.target === nodeId);
  for (const inEdge of incomingEdges) {
    const sourceNode = nodes.find(n => n.id === inEdge.source);
    if (sourceNode?.type === 'branch') return true;
  }

  return false;
}

/**
 * LLM èŠ‚ç‚¹æ‰§è¡Œå™¨
 * è´Ÿè´£æ‰§è¡Œ LLM èŠ‚ç‚¹ï¼Œæ”¯æŒæ­£å¸¸æ¨¡å¼ã€è°ƒè¯•æ¨¡å¼å’Œå¯¹è¯è®°å¿†
 */
export class LLMNodeExecutor extends BaseNodeExecutor {
  async execute(
    node: AppNode,
    context: FlowContext,
    mockData?: Record<string, unknown>
  ): Promise<ExecutionResult> {
    // Merge mockData from argument and context
    const effectiveMockData = mockData || (context.mock as Record<string, unknown>);

    // Quota check: Only in production mode (skip in debug mode with mockData)
    if (!effectiveMockData || Object.keys(effectiveMockData).length === 0) {
      try {
        const user = await authService.getCurrentUser();

        // If user is not authenticated, return error immediately
        if (!user) {
          return {
            output: {
              error: "è¯·å…ˆç™»å½•ä»¥ä½¿ç”¨ LLM åŠŸèƒ½",
            },
            executionTime: 0,
          };
        }

        // Check quota availability
        const quotaCheck = await quotaService.checkQuota(user.id, "llm_executions");
        if (!quotaCheck.allowed) {
          return {
            output: {
              error: `LLM æ‰§è¡Œæ¬¡æ•°å·²ç”¨å®Œ (${quotaCheck.used}/${quotaCheck.limit})ã€‚è¯·è”ç³»ç®¡ç†å‘˜å¢åŠ é…é¢ã€‚`,
            },
            executionTime: 0,
          };
        }
      } catch (e) {
        const errorMsg = e instanceof Error ? e.message : String(e);
        console.error("[LLMNodeExecutor] Quota check failed:", errorMsg);
        // SECURITY FIX: Fail fast instead of degraded mode
        return {
          output: {
            error: "é…é¢æ£€æŸ¥å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•æˆ–è”ç³»æ”¯æŒ",
          },
          executionTime: 0,
        };
      }
    }

    const { result, time } = await this.measureTime(async () => {
      await this.delay(LLM_EXECUTOR_CONFIG.DEFAULT_DELAY_MS);

      const llmData = node.data as LLMNodeData;
      let systemPrompt = llmData.systemPrompt || "";
      let inputContent: string;

      // è·å– flow store çŠ¶æ€ï¼ˆæå‡åˆ°å¤–å±‚ä½œç”¨åŸŸä»¥ä¾¿åç»­å¤ç”¨ï¼‰
      const storeState = useFlowStore.getState();
      const { nodes: allNodes, edges: allEdges, flowContext: globalFlowContext } = storeState;

      // REFACTOR: åªè®¡ç®—ä¸€æ¬¡ isUserFacingLLMï¼Œé¿å…é‡å¤ä»£ç 
      const isUserFacingLLM = checkIsUserFacingLLM(node.id, allNodes, allEdges);

      // è°ƒè¯•æ¨¡å¼ï¼šä½¿ç”¨ mock æ•°æ®æ›¿æ¢å˜é‡
      if (effectiveMockData && Object.keys(effectiveMockData).length > 0) {
        // å°† mockData çš„å€¼è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        const stringValues: Record<string, string> = {};
        Object.entries(effectiveMockData).forEach(([key, value]) => {
          stringValues[key] = String(value);
        });

        // æ›¿æ¢ systemPrompt ä¸­çš„å˜é‡
        systemPrompt = replaceVariables(systemPrompt, stringValues);

        // è¾“å…¥å†…å®¹ä½¿ç”¨ç¬¬ä¸€ä¸ª mock å€¼æˆ–ç©ºå­—ç¬¦ä¸²
        inputContent = Object.values(stringValues)[0] || "";
      } else {
        // å¯¹äºç”¨æˆ·äº¤äº’ LLMï¼Œä»å…¨å±€ flowContext ä¸­è·å– Input èŠ‚ç‚¹çš„åŸå§‹è¾“å…¥
        if (isUserFacingLLM) {
          // FIX: ä¼˜å…ˆä»ç›´æ¥ä¸Šæ¸¸ context ä¸­æŸ¥æ‰¾ Input èŠ‚ç‚¹æ•°æ®
          const upstreamInputEntry = Object.entries(context)
            .filter(([key]) => !key.startsWith('_'))
            .find(([key]) => key.startsWith('input'));

          if (upstreamInputEntry) {
            // ä½¿ç”¨ç›´æ¥ä¸Šæ¸¸çš„ Input èŠ‚ç‚¹æ•°æ®
            const inputNodeData = upstreamInputEntry[1] as Record<string, unknown>;
            inputContent = String(inputNodeData?.user_input || inputNodeData?.text || "");
          } else {
            // å…œåº•ï¼šä»å…¨å±€ flowContext ä¸­æ‰¾åˆ° Input èŠ‚ç‚¹çš„æ•°æ®
            const inputNodeId = Object.keys(globalFlowContext).find(key =>
              !key.startsWith('_') && key.startsWith('input')
            );

            if (inputNodeId) {
              const inputNodeData = globalFlowContext[inputNodeId] as Record<string, unknown>;
              inputContent = String(inputNodeData?.user_input || inputNodeData?.text || "");
            } else {
              // æœ€åå…œåº•ï¼šå°è¯•ä»å…¨å±€ flowContext ä¸­æ‰¾åˆ° user_input
              const entries = Object.entries(globalFlowContext).filter(([k]) => !k.startsWith('_'));
              let foundInput = "";
              for (const [, data] of entries) {
                if (data && typeof data === 'object' && 'user_input' in (data as object)) {
                  foundInput = String((data as Record<string, unknown>).user_input || "");
                  break;
                }
              }
              inputContent = foundInput || extractInputFromContext(context, "Start");
            }
          }
        } else {
          // æ™®é€šæ¨¡å¼ï¼šä½¿ç”¨å…±äº«å·¥å…·å‡½æ•°ä»ä¸Šæ¸¸èŠ‚ç‚¹æå–è¾“å…¥
          inputContent = extractInputFromContext(context, "Start");
        }
      }

      // ========== å¯¹è¯è®°å¿†åŠŸèƒ½ ==========
      let conversationHistory: ConversationMessage[] = [];
      const memoryEnabled = llmData.enableMemory === true;
      const maxTurns = llmData.memoryMaxTurns ?? 10;

      // ä» context ä¸­æå– flowId å’Œ sessionId
      const flowId = (context._meta as Record<string, unknown>)?.flowId as string | undefined;
      const sessionId = (context._meta as Record<string, unknown>)?.sessionId as string | undefined;

      // REFACTOR: é‡ç”¨ä¹‹å‰è®¡ç®—çš„ isUserFacingLLMï¼Œé¿å…é‡å¤å¯¼å…¥å’Œè®¡ç®—
      // ç”¨æˆ·äº¤äº’ LLM ä½¿ç”¨å…±äº«é”® "__main__"ï¼Œä¸­é—´å¤„ç† LLM ä½¿ç”¨è‡ªå·±çš„ node.id
      const memoryNodeId = isUserFacingLLM ? "__main__" : node.id;

      if (memoryEnabled && flowId && sessionId) {
        try {
          // è·å–å†å²å¯¹è¯
          conversationHistory = await llmMemoryService.getHistory(
            flowId,
            memoryNodeId,
            sessionId,
            maxTurns
          );

          // ä¿å­˜å½“å‰ç”¨æˆ·è¾“å…¥åˆ°è®°å¿†
          await llmMemoryService.appendMessage(
            flowId,
            memoryNodeId,
            sessionId,
            'user',
            inputContent
          );
        } catch (e) {
          console.error("[LLMNodeExecutor] Memory fetch failed:", e);
          // è®°å¿†è·å–å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
        }
      }

      try {
        // é‡ç”¨ä¹‹å‰è·å–çš„ store state
        const { appendStreamingText, clearStreaming } = storeState;

        // REFACTOR: é‡ç”¨ä¹‹å‰è®¡ç®—çš„ isUserFacingLLM åˆ¤æ–­æ˜¯å¦éœ€è¦æµå¼è¾“å‡º
        // åªæœ‰ç”¨æˆ·äº¤äº’ LLM æ‰éœ€è¦æµå¼è¾“å‡º
        const enableStreaming = isUserFacingLLM;

        // Clear previous streaming state only if we're going to stream
        if (enableStreaming) {
          // Reset abort flag first (in case previous streaming was aborted)
          storeState.resetStreamingAbort();
          clearStreaming();
        }

        // Use streaming API endpoint
        const resp = await fetch("/api/run-node-stream", {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({
            model: llmData.model || LLM_EXECUTOR_CONFIG.DEFAULT_MODEL,
            systemPrompt,
            temperature: llmData.temperature ?? LLM_EXECUTOR_CONFIG.DEFAULT_TEMPERATURE,
            input: inputContent,
            // ä¼ å…¥å¯¹è¯å†å²
            conversationHistory: memoryEnabled ? conversationHistory : undefined,
          }),
        });

        if (!resp.ok) {
          throw new Error(`API request failed: ${resp.status}`);
        }

        // Handle streaming response
        const reader = resp.body?.getReader();
        if (!reader) {
          throw new Error("No response body");
        }

        const decoder = new TextDecoder();
        let fullResponse = "";

        while (true) {
          const { done, value } = await reader.read();
          if (done) break;

          const chunk = decoder.decode(value, { stream: true });
          const lines = chunk.split("\n");

          for (const line of lines) {
            if (line.startsWith("data: ")) {
              const data = line.slice(6);
              if (data === "[DONE]") continue;

              try {
                const parsed = JSON.parse(data);
                if (parsed.content) {
                  fullResponse += parsed.content;
                  // Only update streaming state for LLM nodes connected to output
                  if (enableStreaming) {
                    appendStreamingText(parsed.content);
                    // Add small delay to slow down streaming for better UX
                    await new Promise(resolve => setTimeout(resolve, 30));
                  }
                }
                if (parsed.error) {
                  throw new Error(parsed.error);
                }
              } catch (e) {
                // Skip malformed JSON chunks
                if (e instanceof SyntaxError) continue;
                throw e;
              }
            }
          }
        }

        // NOTE: We do NOT clear streaming here for success case.
        // The UI components (AppModeOverlay/useFlowChat) will clear it
        // after they have successfully added the final message to their local state.
        // This prevents the "flash" effect where content disappears before reappearing.

        const responseText = fullResponse;

        // ä¿å­˜ assistant å›å¤åˆ°è®°å¿†
        if (memoryEnabled && flowId && sessionId && responseText) {
          try {
            await llmMemoryService.appendMessage(
              flowId,
              memoryNodeId,
              sessionId,
              'assistant',
              responseText
            );

            // ä¿®å‰ªè¶…å‡ºè½®æ•°é™åˆ¶çš„å†å²
            await llmMemoryService.trimHistory(flowId, memoryNodeId, sessionId, maxTurns);
          } catch (e) {
            console.error("[LLMNodeExecutor] Memory save failed:", e);
          }
        }

        // Increment usage quota after successful execution (only in production mode)
        if (!effectiveMockData || Object.keys(effectiveMockData).length === 0) {
          try {
            const user = await authService.getCurrentUser();
            if (user) {
              const updated = await quotaService.incrementUsage(user.id, "llm_executions");
              if (!updated) {
                console.warn("[LLMNodeExecutor] Failed to increment quota - quota service returned null");
              } else {
                // ğŸ§¹ REFACTOR: Auto-refresh quota in UI for immediate feedback
                const { refreshQuota } = useQuotaStore.getState();
                await refreshQuota(user.id);
              }
            } else {
              console.warn("[LLMNodeExecutor] Cannot increment quota - user not authenticated");
            }
          } catch (e) {
            const errorMsg = e instanceof Error ? e.message : String(e);
            console.error("[LLMNodeExecutor] Failed to increment quota:", errorMsg);
            // DEFENSIVE: We don't fail the request here since execution was successful
          }
        }

        return { response: responseText };
      } catch (e) {
        const errorMessage = e instanceof Error ? e.message : String(e);
        console.error("LLM execution failed:", errorMessage);
        // FIX: æ‰§è¡Œå¤±è´¥æ—¶æ¸…ç†æµå¼è¾“å‡ºçŠ¶æ€
        if (isUserFacingLLM) {
          storeState.clearStreaming();
        }
        return { error: errorMessage };
      }
    });

    return {
      output: result,
      executionTime: time
    };
  }
}
